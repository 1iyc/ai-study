# 01. MLP & FFNN

Multi Layer Perceptron & Feed Forwad Neural Network

## λ‹¨μΌ νΌμ…‰νΈλ΅ 

* λ‡μ λ‰΄λ°μ„ λ¨λ°©
* μΈν’‹κ³Ό κ°€μ¤‘μΉλ“¤μ κ³±(w0x0, w1x1, w2x2)λ“¤μ ν•©μ— Bias κ°’μ„ λ”ν•λ‹¤
* μ„μ κ°’μ„ Activation Function(ν™μ„± ν•¨μ)μ— λ„£μ–΄μ„ κ²°κ³Ό μ¶λ ¥

## Activation function

* λ‰΄λ°μ—μ„ κ³„μ‚°λ κ°’μ΄ μ„κ³„μΉλ³΄λ‹¤ ν¬λ©΄ 1 μ¶λ ¥ μ‘μΌλ©΄ 0 μ¶λ ¥
* λ‹¤μμΌλ΅ μ‹ νΈλ¥Ό λ³΄λ‚΄λ”μ§€ κ²°μ •

## κ°€μ¤‘μΉμ™€ Bias μ΅°μ •

* β£λ©ν‘κ°’ - μ¶λ ¥κ°’ β£* μΈν’‹κ°’ * learning rate + W(t) = W(t+1)
* Biasλ” W(t)λ¥Ό B(t)λ΅ λ³€κ²½
* λ³€ν™”κ°€ μ—†κ±°λ‚ λ―Έλ―Έν•  μ •λ„ κΉμ§€ μΈν’‹μ„ κ³„μ† ν•™μµ

## Tutorial and.py

## λ‹¤μΈµ νΌμ…‰νΈλ΅ 

* μ…λ ¥μΈµκ³Ό μ¶λ ¥μΈµ μ‚¬μ΄μ— 1κ° μ΄μƒμ hidden layer μ¶”κ°€
* λ€λ¶€λ¶„ μ—­μ „ν μ•κ³ λ¦¬μ¦μΌλ΅ ν•™μµ

## μ—­μ „ν

* lossλ¥Ό μ¤„μ΄λ” κ²ƒμ΄ λ©ν‘
* w = w - learning rate * (π•E / π•w)

## MLPμ ν•κ³„



## Activation function

* λ„¤νΈμ›ν¬μ— λΉ„μ„ ν•μ„ μ¶”κ°€ν•κΈ°μ„ν•΄ μ‚¬μ©λ¨
* 


